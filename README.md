# ğŸ§  Text Generation Model using GPT-2

## ğŸ¯ Objective
This project builds a text generation model using Python and the GPT-2 pre-trained model. It takes an input prompt from the user and generates a creative and grammatically correct continuation of that sentence.

---

## ğŸ“š Dataset
- Source: Project Gutenberg (via NLTK)
- Text Used: *Emma* by Jane Austen
- Preprocessing: Lowercased, special characters removed, sentence tokenization

---

## âš™ï¸ Technologies Used
- Python
- HuggingFace Transformers
- PyTorch
- NLTK

---

## ğŸ’¡ Features
- Real-time prompt-based text generation
- Optional perplexity evaluation (a measure of how well the model predicts text)
- Preprocessed dataset saved for inspection

---

## ğŸ§ª Sample Prompts
Try prompts like:
- "Once upon a time"
- "In the distant future, humanity"
- "She opened the door and saw"

---

## ğŸ§ª Evaluation
- Perplexity Score (lower = better)
- User can test perplexity on any custom prompt

---

## â–¶ï¸ How to Run

### 1. Install requirements:
```bash
pip install -r requirements.txt