# 🧠 Text Generation Model using GPT-2

## 🎯 Objective
This project builds a text generation model using Python and the GPT-2 pre-trained model. It takes an input prompt from the user and generates a creative and grammatically correct continuation of that sentence.

---

## 📚 Dataset
- Source: Project Gutenberg (via NLTK)
- Text Used: *Emma* by Jane Austen
- Preprocessing: Lowercased, special characters removed, sentence tokenization

---

## ⚙️ Technologies Used
- Python
- HuggingFace Transformers
- PyTorch
- NLTK

---

## 💡 Features
- Real-time prompt-based text generation
- Optional perplexity evaluation (a measure of how well the model predicts text)
- Preprocessed dataset saved for inspection

---

## 🧪 Sample Prompts
Try prompts like:
- "Once upon a time"
- "In the distant future, humanity"
- "She opened the door and saw"

---

## 🧪 Evaluation
- Perplexity Score (lower = better)
- User can test perplexity on any custom prompt

---

## ▶️ How to Run

### 1. Install requirements:
```bash
pip install -r requirements.txt